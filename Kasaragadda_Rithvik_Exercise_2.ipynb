{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''### Research Question:\n",
        "**What is life without technology?**\n",
        "\n",
        "This study explores how life changes when people give up modern technology, especially digital devices like smartphones and computers. The aim is to understand how this affects well-being, productivity, and social interactions. By examining how people adapt without technology, we can gain insights into its impact on daily life.\n",
        "\n",
        "### Data Collection Strategy:\n",
        "\n",
        "To answer this question, both **subjective** (self-reported) and **objective** (observational) data are needed. Participants will abstain from technology for a specific period (e.g., 1-4 weeks), and data will be collected on how this affects their routine, mental state, and social interactions.\n",
        "\n",
        "#### 1. **Mental and Emotional Well-being Data**\n",
        "   - **Variables**: Participants report their mental health, stress, and mood.\n",
        "   - **Tool**: Daily or weekly surveys (via an app before and after the detox, or manually during the detox).\n",
        "   - **Frequency**: Daily or weekly.\n",
        "   - **Sample Size**: 100-200 participants for 1 month.\n",
        "   - **Amount**: 4,000 entries (100 participants x 30 days).\n",
        "\n",
        "#### 2. **Productivity and Daily Activities Data**\n",
        "   - **Variables**: Time spent on activities like work, hobbies, or face-to-face interactions.\n",
        "   - **Tool**: Activity logs (written during the detox, or digital before and after).\n",
        "   - **Frequency**: Daily.\n",
        "   - **Sample Size**: Same as above.\n",
        "   - **Amount**: 3,000 entries.\n",
        "\n",
        "#### 3. **Social Interaction Data**\n",
        "   - **Variables**: Type and frequency of social interactions (e.g., phone calls, in-person meetings).\n",
        "   - **Tool**: Weekly logs to track the quality and quantity of social interactions.\n",
        "   - **Frequency**: Weekly.\n",
        "   - **Sample Size**: Same as above.\n",
        "   - **Amount**: 400 entries.\n",
        "\n",
        "#### 4. **Physiological Data (optional)**\n",
        "   - **Variables**: Sleep quality, heart rate variability, and physical activity.\n",
        "   - **Tool**: Wearables like smartwatches (optional).\n",
        "   - **Sample Size**: A subset of 50-100 participants.\n",
        "\n",
        "---\n",
        "\n",
        "### Steps for Collecting and Saving Data:\n",
        "\n",
        "#### **Step 1: Recruit Participants**\n",
        "- Recruit a diverse group of participants who are willing to give up technology for a set period. This group should include individuals from different age groups, work backgrounds (e.g., remote workers, students), and lifestyles.\n",
        "\n",
        "#### **Step 2: Set Up the Data Collection Platform**\n",
        "- Use Google Forms or paper-based surveys to collect data on well-being, activities, and social interactions before and after the detox.\n",
        "- Provide participants with physical notebooks to manually record their daily activities and feelings during the detox.\n",
        "\n",
        "#### **Step 3: Collect Pre-Detox Baseline Data**\n",
        "- Before the detox begins, collect baseline data on participants’ current technology use (screen time, social media hours, etc.), mental health, and productivity.\n",
        "\n",
        "#### **Step 4: Collect Data During the Detox**\n",
        "- Participants abstain from technology for 1-4 weeks. During this period, they will record their daily activities, feelings, and social interactions manually.\n",
        "- Collect daily logs, with participants noting changes in their routine, stress levels, and emotional state.\n",
        "\n",
        "#### **Step 5: Post-Detox Data Collection**\n",
        "- After the detox, participants will resume using the digital tools to submit final surveys reflecting on their experiences and changes in well-being and productivity.\n",
        "\n",
        "#### **Step 6: Manage Data Storage and Backup**\n",
        "- Store the data securely in a cloud-based storage solution (e.g., Google Cloud).\n",
        "- Perform regular backups and check the quality of the collected data to ensure completeness.\n",
        "\n",
        "#### **Step 7: Data Security and Anonymization**\n",
        "- De-identify all data by assigning participants unique codes to protect their privacy.\n",
        "- Ensure compliance with data protection regulations like GDPR.\n",
        "\n",
        "---\n",
        "\n",
        "### Data Analysis:\n",
        "1. **Compare Well-being**: Analyze how participants’ well-being (stress, mood, mental health) changed during the technology-free period compared to their baseline period.\n",
        "2. **Analyze Productivity**: Examine any shifts in daily productivity and work habits without technology.\n",
        "3. **Social Interaction Trends**: Assess whether participants had more or higher-quality face-to-face social interactions during the detox.\n",
        "4. **Qualitative Insights**: Review open-ended responses on how participants felt about their experience, the challenges, and benefits.\n",
        "\n",
        "By analyzing both quantitative and qualitative data, the study will reveal how living without technology impacts people's lives.\n",
        "'''"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''To collect a dataset of 1000 samples for the question \"How does the daily exposure to green spaces influence mental well-being and productivity in a remote-working population?\", you can simulate a data collection process in Python. This involves generating random data based on certain assumptions for mental well-being, productivity, and time spent in green spaces.\n",
        "\n",
        "Here’s a simple Python code to generate a dataset of 1000 samples, where each sample includes:\n",
        "\n",
        "Time spent in green spaces (in minutes).\n",
        "Mental well-being score (1-10).\n",
        "Productivity score (1-10).\n",
        "We'll assume the well-being and productivity scores are influenced by the time spent in green spaces.'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to generate mental well-being and productivity based on time spent in green spaces\n",
        "def generate_wellbeing_productivity(time_in_green_space):\n",
        "    # Assume mental well-being increases with more time spent in green spaces\n",
        "    wellbeing = np.clip(np.random.normal(loc=time_in_green_space / 30 + 5, scale=1), 1, 10)\n",
        "\n",
        "    # Assume productivity peaks at moderate exposure (e.g., 60-90 minutes) and decreases if too much/too little\n",
        "    productivity = np.clip(np.random.normal(loc=-0.01 * (time_in_green_space - 75)**2 + 8, scale=1), 1, 10)\n",
        "\n",
        "    return wellbeing, productivity\n",
        "\n",
        "# Parameters for the dataset\n",
        "num_samples = 1000\n",
        "\n",
        "# Simulate data\n",
        "data = {\n",
        "    \"Participant_ID\": np.arange(1, num_samples + 1),\n",
        "    \"Time_in_Green_Space\": np.random.randint(0, 180, num_samples),  # Random time between 0 and 180 minutes\n",
        "}\n",
        "\n",
        "# Initialize lists to store wellbeing and productivity scores\n",
        "wellbeing_scores = []\n",
        "productivity_scores = []\n",
        "\n",
        "# Generate wellbeing and productivity scores based on time spent in green spaces\n",
        "for time in data[\"Time_in_Green_Space\"]:\n",
        "    wellbeing, productivity = generate_wellbeing_productivity(time)\n",
        "    wellbeing_scores.append(wellbeing)\n",
        "    productivity_scores.append(productivity)\n",
        "\n",
        "# Add the generated scores to the dataset\n",
        "data[\"Mental_Wellbeing\"] = wellbeing_scores\n",
        "data[\"Productivity\"] = productivity_scores\n",
        "\n",
        "# Convert the data to a pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Show the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Save to a CSV file\n",
        "df.to_csv('green_space_wellbeing_productivity_dataset.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373df302-a141-4008-bd29-bde6119c36e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 1000 articles and saved to scholar_articles.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_scholar_articles(keyword, num_articles):\n",
        "    articles = []\n",
        "    url = f\"https://scholar.google.com/scholar?q={keyword}&hl=en&as_sdt=0,5&as_ylo=2014&as_yhi=2024\"\n",
        "\n",
        "    while len(articles) < num_articles:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        for entry in soup.find_all('div', class_='gs_ri'):\n",
        "            title = entry.find('h3', class_='gs_rt').text\n",
        "            venue = entry.find('div', class_='gs_a').text.split('-')[1].strip()\n",
        "            year = entry.find('div', class_='gs_a').text.split('-')[-1].strip().split()[-1]\n",
        "            authors = entry.find('div', class_='gs_a').text.split('-')[0].strip()\n",
        "            abstract = entry.find('div', class_='gs_rs').text if entry.find('div', class_='gs_rs') else \"No abstract available\"\n",
        "\n",
        "            articles.append({\n",
        "                'Title': title,\n",
        "                'Venue': venue,\n",
        "                'Year': year,\n",
        "                'Authors': authors,\n",
        "                'Abstract': abstract\n",
        "            })\n",
        "\n",
        "            if len(articles) >= num_articles:\n",
        "                break\n",
        "\n",
        "        next_button = soup.find('td', class_='b d6cvqb')\n",
        "        if next_button and len(articles) < num_articles:\n",
        "            url = \"https://scholar.google.com\" + next_button.a['href']\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Collect 1000 articles with the keyword 'XYZ'\n",
        "articles = get_scholar_articles('XYZ', 1000)\n",
        "\n",
        "# Convert the list of articles to a DataFrame and save it as a CSV file\n",
        "df = pd.DataFrame(articles)\n",
        "df.to_csv('scholar_articles.csv', index=False)\n",
        "\n",
        "print(\"Collected 1000 articles and saved to scholar_articles.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": [
        "!pip install tweepy\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "# I dont have any developer API's from twitter or any oter social media i applied for it waiting for reply\n",
        "API_KEY = 'api_key'\n",
        "API_SECRET_KEY = 'api_secret_key'\n",
        "ACCESS_TOKEN = 'access_token'\n",
        "ACCESS_TOKEN_SECRET = 'access_token_secret'\n",
        "\n",
        "# Authenticate to Twitter API\n",
        "auth = tweepy.OAuthHandler(API_KEY, API_SECRET_KEY)\n",
        "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "\n",
        "hashtag = \"#climatechange\"\n",
        "tweet_count = 100\n",
        "\n",
        "# Collect tweets\n",
        "tweets_data = []\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\", tweet_mode=\"extended\").items(tweet_count):\n",
        "    tweets_data.append({\n",
        "        'Tweet ID': tweet.id_str,\n",
        "        'Username': tweet.user.screen_name,\n",
        "        'Tweet Text': tweet.full_text,\n",
        "        'Created At': tweet.created_at,\n",
        "        'Likes': tweet.favorite_count,\n",
        "        'Retweets': tweet.retweet_count\n",
        "    })\n",
        "\n",
        "# Convert collected tweets data to a DataFrame\n",
        "df = pd.DataFrame(tweets_data)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Save to CSV file\n",
        "df.to_csv('twitter_hashtag_data.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}