{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 3**\n",
        "\n",
        "In this assignment, we will delve into various aspects of natural language processing (NLP) and text analysis. The tasks are designed to deepen your understanding of key NLP concepts and techniques, as well as to provide hands-on experience with practical applications.\n",
        "\n",
        "Through these tasks, you'll gain practical experience in NLP techniques such as N-gram analysis, TF-IDF, word embedding model creation, and sentiment analysis dataset creation.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: See Canvas\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "## Question 1 (30 points)\n",
        "\n",
        "**Understand N-gram**\n",
        "\n",
        "Write a python program to conduct N-gram analysis based on the dataset in your assignment two. You need to write codes from scratch instead of using any pre-existing libraries to do so:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the noun phrases and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9v8IikDpqrxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823c7b68-09d9-4cdf-8c38-b7f2b8267e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-gram Counts (N=3): Counter({'this is the': 2, 'review it is': 2, 'is the first': 1, 'the first review': 1, 'first review it': 1, 'it is really': 1, 'is really good': 1, 'is the second': 1, 'the second review': 1, 'second review it': 1, 'it is not': 1, 'is not as': 1, 'not as good': 1, 'as good as': 1, 'good as the': 1, 'as the first': 1, 'the first one': 1})\n",
            "Bigram Probabilities: {'this is': 1.0, 'is the': 0.5, 'the first': 0.6666666666666666, 'first review': 0.5, 'review it': 1.0, 'it is': 1.0, 'is really': 0.25, 'really good': 1.0, 'the second': 0.3333333333333333, 'second review': 1.0, 'is not': 0.25, 'not as': 1.0, 'as good': 0.5, 'good as': 0.5, 'as the': 0.5, 'first one': 0.5}\n",
            "Review this is the\tfirst review\tit is really\tgood\tsecond review\tit is not\tas good as\tthe first one\n",
            "Review 1\t0.50\t0.50\t0.50\t0.50\t0.00\t0.00\t0.00\t0.00\n",
            "Review 2\t0.50\t0.00\t0.00\t0.50\t0.50\t0.50\t0.50\t0.50\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Sample dataset\n",
        "dataset = [\n",
        "    \"This is the first review. It is really good.\",\n",
        "    \"This is the second review. It is not as good as the first one.\",\n",
        "\n",
        "]\n",
        "\n",
        "# Function to generate N-grams\n",
        "def generate_ngrams(text, n):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n",
        "\n",
        "# Function to count N-grams\n",
        "def count_ngrams(dataset, n):\n",
        "    ngram_counts = Counter()\n",
        "    for text in dataset:\n",
        "        ngrams = generate_ngrams(text, n)\n",
        "        ngram_counts.update(ngrams)\n",
        "    return ngram_counts\n",
        "\n",
        "# Function to calculate bigram probabilities\n",
        "def calculate_bigram_probabilities(dataset):\n",
        "    bigram_counts = count_ngrams(dataset, 2)\n",
        "    unigram_counts = count_ngrams(dataset, 1)\n",
        "    bigram_probabilities = {}\n",
        "    for bigram in bigram_counts:\n",
        "        w1, w2 = bigram.split()\n",
        "        bigram_probabilities[bigram] = bigram_counts[bigram] / unigram_counts[w1]\n",
        "    return bigram_probabilities\n",
        "\n",
        "# Function to extract noun phrases (simple heuristic)\n",
        "def extract_noun_phrases(text):\n",
        "    # This is a very basic heuristic for noun phrase extraction\n",
        "    noun_phrases = re.findall(r'\\b(?:\\w+\\s+){0,2}\\w+\\b', text.lower())\n",
        "    return noun_phrases\n",
        "\n",
        "# Function to calculate relative probabilities of noun phrases\n",
        "def calculate_relative_probabilities(dataset):\n",
        "    noun_phrase_counts = Counter()\n",
        "    for text in dataset:\n",
        "        noun_phrases = extract_noun_phrases(text)\n",
        "        noun_phrase_counts.update(noun_phrases)\n",
        "\n",
        "    max_frequency = max(noun_phrase_counts.values())\n",
        "    relative_probabilities = {phrase: count / max_frequency for phrase, count in noun_phrase_counts.items()}\n",
        "\n",
        "    return relative_probabilities, max_frequency\n",
        "\n",
        "# Function to print results in a table\n",
        "def print_results_table(dataset):\n",
        "    relative_probabilities, max_frequency = calculate_relative_probabilities(dataset)\n",
        "    noun_phrases = list(relative_probabilities.keys())\n",
        "\n",
        "    print(\"Review\", \"\\t\".join(noun_phrases))\n",
        "    for i, text in enumerate(dataset):\n",
        "        row = [f\"Review {i+1}\"]\n",
        "        for phrase in noun_phrases:\n",
        "            count = text.lower().count(phrase)\n",
        "            row.append(f\"{count / max_frequency:.2f}\")\n",
        "        print(\"\\t\".join(row))\n",
        "\n",
        "# Main execution\n",
        "ngram_counts = count_ngrams(dataset, 3)\n",
        "bigram_probabilities = calculate_bigram_probabilities(dataset)\n",
        "print(\"N-gram Counts (N=3):\", ngram_counts)\n",
        "print(\"Bigram Probabilities:\", bigram_probabilities)\n",
        "print_results_table(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "## Question 2 (25 points)\n",
        "\n",
        "**Undersand TF-IDF and Document representation**\n",
        "\n",
        "Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program:\n",
        "\n",
        "(1) To build the documents-terms weights (tf * idf) matrix.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using cosine similarity.\n",
        "\n",
        "Note: You need to write codes from scratch instead of using any pre-existing libraries to do so."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# New sample dataset\n",
        "documents = [\n",
        "    \"An outstanding movie with a haunting performance and best character development.\",\n",
        "    \"The movie had a great storyline and the character development was superb.\",\n",
        "    \"A haunting performance by the lead actor made this movie unforgettable.\",\n",
        "    \"Best character development and an outstanding movie experience.\",\n",
        "    # Add more documents here\n",
        "]\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Function to calculate term frequency (TF)\n",
        "def compute_tf(doc):\n",
        "    tf = Counter(tokenize(doc))\n",
        "    total_terms = len(tokenize(doc))\n",
        "    for term in tf:\n",
        "        tf[term] /= total_terms\n",
        "    return tf\n",
        "\n",
        "# Function to calculate inverse document frequency (IDF)\n",
        "def compute_idf(docs):\n",
        "    idf = defaultdict(lambda: 0)\n",
        "    total_docs = len(docs)\n",
        "    for doc in docs:\n",
        "        terms = set(tokenize(doc))\n",
        "        for term in terms:\n",
        "            idf[term] += 1\n",
        "    for term in idf:\n",
        "        idf[term] = math.log(total_docs / idf[term])\n",
        "    return idf\n",
        "\n",
        "# Function to calculate TF-IDF\n",
        "def compute_tfidf(docs):\n",
        "    idf = compute_idf(docs)\n",
        "    tfidf = []\n",
        "    for doc in docs:\n",
        "        tf = compute_tf(doc)\n",
        "        tfidf_doc = {term: tf[term] * idf[term] for term in tf}\n",
        "        tfidf.append(tfidf_doc)\n",
        "    return tfidf\n",
        "\n",
        "# Function to build the documents-terms weights (tf * idf) matrix\n",
        "def build_tfidf_matrix(docs):\n",
        "    tfidf = compute_tfidf(docs)\n",
        "    terms = set(term for doc in tfidf for term in doc)\n",
        "    matrix = []\n",
        "    for doc in tfidf:\n",
        "        row = [doc.get(term, 0) for term in terms]\n",
        "        matrix.append(row)\n",
        "    return matrix, list(terms)\n",
        "\n",
        "# Function to compute cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
        "    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n",
        "    magnitude2 = math.sqrt(sum(a * a for a in vec2))\n",
        "    if not magnitude1 or not magnitude2:\n",
        "        return 0.0\n",
        "    return dot_product / (magnitude1 * magnitude2)\n",
        "\n",
        "# Function to rank documents based on a query\n",
        "def rank_documents(query, docs, terms, idf):\n",
        "    query_tfidf = compute_tf(query)\n",
        "    query_vector = [query_tfidf.get(term, 0) * idf.get(term, 0) for term in terms]\n",
        "    print(\"Query Vector:\", query_vector)  # Debug print\n",
        "    similarities = []\n",
        "    for doc_vector in docs:\n",
        "        similarity = cosine_similarity(query_vector, doc_vector)\n",
        "        similarities.append(similarity)\n",
        "    ranked_docs = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
        "    return ranked_docs\n",
        "\n",
        "# Main execution\n",
        "tfidf_matrix, terms = build_tfidf_matrix(documents)\n",
        "idf_values = compute_idf(documents)  # Compute IDF values separately\n",
        "print(\"TF-IDF Matrix:\")\n",
        "for row in tfidf_matrix:\n",
        "    print(row)\n",
        "print(\"Terms:\", terms)\n",
        "\n",
        "# Example query\n",
        "query = \"An outstanding movie with a haunting performance and best character development\"\n",
        "\n",
        "ranked_docs = rank_documents(query, tfidf_matrix, terms, idf_values)\n",
        "print(\"Ranked Documents:\")\n",
        "for index, similarity in ranked_docs:\n",
        "    print(f\"Document {index + 1}: Similarity = {similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP_fdBwcjgJX",
        "outputId": "59f3a783-c886-486a-8305-8f81e96e0d54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[0.0, 0.06301338005090412, 0, 0.12602676010180824, 0, 0.026152915677434625, 0.06301338005090412, 0.06301338005090412, 0, 0.12602676010180824, 0.026152915677434625, 0, 0.06301338005090412, 0, 0, 0, 0, 0, 0.026152915677434625, 0, 0.06301338005090412, 0, 0, 0, 0]\n",
            "[0.0, 0, 0, 0, 0, 0.023973506037648404, 0, 0, 0, 0, 0.023973506037648404, 0.057762265046662105, 0, 0.11552453009332421, 0.11552453009332421, 0, 0, 0, 0.023973506037648404, 0.11552453009332421, 0, 0, 0.11552453009332421, 0.11552453009332421, 0.11552453009332421]\n",
            "[0.0, 0, 0.12602676010180824, 0, 0, 0, 0, 0.06301338005090412, 0.12602676010180824, 0, 0.026152915677434625, 0, 0.06301338005090412, 0, 0.06301338005090412, 0.12602676010180824, 0.12602676010180824, 0.12602676010180824, 0, 0, 0, 0.12602676010180824, 0, 0, 0]\n",
            "[0.0, 0.08664339756999316, 0, 0, 0.17328679513998632, 0.035960259056472606, 0.08664339756999316, 0, 0, 0, 0, 0.08664339756999316, 0, 0, 0, 0, 0, 0, 0.035960259056472606, 0, 0.08664339756999316, 0, 0, 0, 0]\n",
            "Terms: ['movie', 'best', 'this', 'with', 'experience.', 'character', 'outstanding', 'haunting', 'unforgettable.', 'development.', 'a', 'development', 'performance', 'great', 'the', 'made', 'actor', 'lead', 'and', 'storyline', 'an', 'by', 'was', 'superb.', 'had']\n",
            "Query Vector: [0.0, 0.06301338005090412, 0.0, 0.12602676010180824, 0.0, 0.026152915677434625, 0.06301338005090412, 0.06301338005090412, 0.0, 0.0, 0.026152915677434625, 0.06301338005090412, 0.06301338005090412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026152915677434625, 0.0, 0.06301338005090412, 0.0, 0.0, 0.0, 0.0]\n",
            "Ranked Documents:\n",
            "Document 1: Similarity = 0.7982\n",
            "Document 4: Similarity = 0.4638\n",
            "Document 3: Similarity = 0.1285\n",
            "Document 2: Similarity = 0.0926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "## Question 3 (25 points)\n",
        "\n",
        "**Create your own word embedding model**\n",
        "\n",
        "Use the data you collected for assignment 2 to build a word embedding model:\n",
        "\n",
        "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
        "\n",
        "(2) Visualize the word embedding model you created.\n",
        "\n",
        "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "\n",
        "Reference: https://jaketae.github.io/study/word2vec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eczZgyAoo05Q"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDoVp3aYoU8F"
      },
      "source": [
        "## Question 4 (20 Points)\n",
        "\n",
        "**Create your own training and evaluation data for sentiment analysis.**\n",
        "\n",
        " **You don't need to write program for this question!**\n",
        "\n",
        " For example, if you collected a movie review or a product review data, then you can do the following steps:\n",
        "\n",
        "*   Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral).\n",
        "\n",
        "*   Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew.\n",
        "\n",
        "*   This datset will be used for assignment four: sentiment analysis and text classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyK54UY6ompS"
      },
      "outputs": [],
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "\n",
        "# Link:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNXlsbrirHRo"
      },
      "outputs": [],
      "source": [
        "# Type your answer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}